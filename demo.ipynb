{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Demo of the pipeline\n",
   "id": "7cac6f5a02281e53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T23:17:36.587667Z",
     "start_time": "2025-01-12T23:17:36.582071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ],
   "id": "95c9be7f6536046a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ezraapple/Projects/dubbing_demo\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Load Demo Video",
   "id": "9fdb29b561fbac60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T23:17:37.163762Z",
     "start_time": "2025-01-12T23:17:36.589103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "video_path = \"demo_video.mov\"\n",
    "\n",
    "file_size_mb = os.path.getsize(video_path) / (1024 * 1024)\n",
    "clip = VideoFileClip(video_path)\n",
    "\n",
    "duration = clip.duration\n",
    "width, height = clip.size  \n",
    "fps = clip.fps  \n",
    "\n",
    "print(f\"File Name: {os.path.basename(video_path)}\")\n",
    "print(f\"File Size: {file_size_mb:.2f} MB\")\n",
    "print(f\"Video Duration: {duration:.2f} seconds\")\n",
    "print(f\"Resolution: {width}x{height}\")\n",
    "print(f\"FPS (if available): {fps}\")\n"
   ],
   "id": "e69f496692932a8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: demo_video.mov\n",
      "File Size: 13.83 MB\n",
      "Video Duration: 10.80 seconds\n",
      "Resolution: 1280x720\n",
      "FPS (if available): 30.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transcribe Video",
   "id": "f3d1828d96df8bb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T23:17:39.122234Z",
     "start_time": "2025-01-12T23:17:37.164652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from transcribe import Transcriber\n",
    "\n",
    "# 1) Extract audio from the video (using MoviePy for quick demo).\n",
    "video_path = \"demo_video.mov\"\n",
    "output_audio_path = \"demo_audio.wav\"\n",
    "\n",
    "clip = VideoFileClip(video_path)\n",
    "clip.audio.write_audiofile(output_audio_path, codec='pcm_s16le', logger=None)  # Writes a WAV by default\n",
    "\n",
    "# 2) Create a Transcriber instance\n",
    "transcriber = Transcriber(model_name=\"base\", device=\"cpu\")\n",
    "\n",
    "# 3) Transcribe the extracted audio\n",
    "result = transcriber.transcribe_audio(output_audio_path)\n",
    "\n",
    "# 4) Display the results\n",
    "transcriber.display_full_text(result)"
   ],
   "id": "1495653869253d57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: English\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1080/1080 [00:00<00:00, 2765.07frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Transcribed Text ---\n",
      " Hello, my name is Ezra and I'm trying to demo this cool technology. The quick brown fox jumps over the lazy dog.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Translate Text",
   "id": "5b60c05ca7591ccd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T23:17:39.877882Z",
     "start_time": "2025-01-12T23:17:39.123763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from translate import Translator\n",
    "\n",
    "# Example transcribed text from the previous step (or supply your own)\n",
    "transcribed_text = result.get(\"text\", \"\")\n",
    "\n",
    "# Create a Translator for English-to-Spanish\n",
    "translator = Translator(source_lang=\"en\", target_lang=\"es\")\n",
    "\n",
    "# Perform the translation\n",
    "translated_text = translator.translate_text(transcribed_text)\n",
    "\n",
    "# Print the result\n",
    "print(\"Original Text:\", transcribed_text)\n",
    "print(\"Translated Text:\", translated_text)"
   ],
   "id": "69c7eafdb8276b47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  Hello, my name is Ezra and I'm trying to demo this cool technology. The quick brown fox jumps over the lazy dog.\n",
      "Translated Text: Hola, me llamo Ezra y estoy tratando de demoler esta tecnología genial. El zorro marrón salta sobre el perrito perezoso.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Speak the Text",
   "id": "22aa4e222c032bd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T23:17:39.880446Z",
     "start_time": "2025-01-12T23:17:39.878796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# In demo.ipynb\n",
    "# \n",
    "# from moviepy.editor import VideoFileClip\n",
    "# from speech import Speaker\n",
    "# \n",
    "# \n",
    "# # Extract audio from demo_video.mov if you haven't done so yet:\n",
    "# video_path = \"demo_video.mov\"\n",
    "# extracted_audio_path = \"demo_audio.wav\"\n",
    "# \n",
    "# clip = VideoFileClip(video_path)\n",
    "# clip.audio.write_audiofile(extracted_audio_path, codec=\"pcm_s16le\")\n",
    "# \n",
    "# # Initialize the Speaker with Spanish output\n",
    "# speaker = Speaker(\n",
    "#     model_name=\"tts_models/multilingual/multi-dataset/xtts_v2\",\n",
    "#     language_code=\"es\"\n",
    "# )"
   ],
   "id": "3c782839139715f7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T23:17:39.882585Z",
     "start_time": "2025-01-12T23:17:39.881175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate speech using the speaker's voice from the extracted audio\n",
    "# output_speech_path = speaker.generate_speech(\n",
    "#     text=translated_text,\n",
    "#     speaker_wav=extracted_audio_path,\n",
    "#     output_path=\"demo_tts.wav\"\n",
    "# )\n",
    "# \n",
    "# print(f\"Cloned speech saved to: {output_speech_path}\")"
   ],
   "id": "6c5e2bf58ba9b0e2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Match the Lips",
   "id": "c3cc3ae687e22a3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T23:44:37.368686Z",
     "start_time": "2025-01-12T23:44:37.364783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# demo.ipynb\n",
    "\n",
    "from lipmatch import LipMatcher\n",
    "\n",
    "# Paths to your files\n",
    "original_video_path = \"demo_video.mp4\"  # The original video with the speaker\n",
    "new_audio_path = \"demo_tts.wav\"        # The newly generated Spanish audio\n",
    "output_video_path = \"final_demo_lipsynced.mp4\"\n",
    "\n",
    "# 1) Create a LipMatcher instance\n",
    "matcher = LipMatcher(\n",
    "    wav2lip_repo_path=\"Wav2Lip\", \n",
    "    checkpoint_path=\"Wav2Lip/checkpoints/wav2lip.pth\"\n",
    ")"
   ],
   "id": "4bcf40ba89cd22bc",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T23:45:46.489019Z",
     "start_time": "2025-01-12T23:44:37.749531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 2) Run the lip synchronization\n",
    "synced_video = matcher.match_lips(\n",
    "    original_video_path=original_video_path, \n",
    "    new_audio_path=new_audio_path, \n",
    "    output_video_path=output_video_path,\n",
    "    pads=[0, 20, 0, 0],\n",
    "    no_smoothing=True\n",
    ")\n",
    "\n",
    "print(f\"Final lip-synced video: {synced_video}\")"
   ],
   "id": "fc4d2e17c35c4638",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Wav2Lip command:\n",
      "python Wav2Lip/inference.py --checkpoint_path Wav2Lip/checkpoints/wav2lip.pth --face demo_video.mp4 --audio demo_tts.wav --outfile final_demo_lipsynced.mp4 --pads 0 20 0 0 --nosmooth\n",
      "Using mps for inference.\n",
      "Reading video frames...\n",
      "Number of frames available for inference: 322\n",
      "(80, 839)\n",
      "Length of mel chunks: 311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001B[A\n",
      "  5%|▌         | 1/20 [00:03<01:10,  3.70s/it]\u001B[A\n",
      " 10%|█         | 2/20 [00:06<00:58,  3.24s/it]\u001B[A\n",
      " 15%|█▌        | 3/20 [00:09<00:55,  3.26s/it]\u001B[A\n",
      " 20%|██        | 4/20 [00:13<00:52,  3.31s/it]\u001B[A\n",
      " 25%|██▌       | 5/20 [00:16<00:49,  3.31s/it]\u001B[A\n",
      " 30%|███       | 6/20 [00:19<00:45,  3.28s/it]\u001B[A\n",
      " 35%|███▌      | 7/20 [00:23<00:43,  3.31s/it]\u001B[A\n",
      " 40%|████      | 8/20 [00:26<00:39,  3.30s/it]\u001B[A\n",
      " 45%|████▌     | 9/20 [00:29<00:34,  3.16s/it]\u001B[A\n",
      " 50%|█████     | 10/20 [00:32<00:31,  3.15s/it]\u001B[A\n",
      " 55%|█████▌    | 11/20 [00:35<00:28,  3.16s/it]\u001B[A\n",
      " 60%|██████    | 12/20 [00:38<00:25,  3.18s/it]\u001B[A\n",
      " 65%|██████▌   | 13/20 [00:41<00:22,  3.15s/it]\u001B[A\n",
      " 70%|███████   | 14/20 [00:44<00:18,  3.03s/it]\u001B[A\n",
      " 75%|███████▌  | 15/20 [00:47<00:15,  3.08s/it]\u001B[A\n",
      " 80%|████████  | 16/20 [00:50<00:12,  3.08s/it]\u001B[A\n",
      " 85%|████████▌ | 17/20 [00:54<00:09,  3.17s/it]\u001B[A\n",
      " 90%|█████████ | 18/20 [00:57<00:06,  3.19s/it]\u001B[A\n",
      " 95%|█████████▌| 19/20 [01:00<00:03,  3.18s/it]\u001B[A\n",
      "100%|██████████| 20/20 [01:02<00:00,  3.11s/it]\u001B[A\n",
      "/Users/ezraapple/Projects/dubbing_demo/Wav2Lip/inference.py:165: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path,\n",
      "100%|██████████| 3/3 [01:05<00:00, 21.77s/it]\n",
      "ffmpeg version 7.1 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with clang version 18.1.8\n",
      "  configuration: --prefix=/Users/runner/miniforge3/conda-bld/ffmpeg_1735647017461/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pl --cc=arm64-apple-darwin20.0.0-clang --cxx=arm64-apple-darwin20.0.0-clang++ --nm=arm64-apple-darwin20.0.0-nm --ar=arm64-apple-darwin20.0.0-ar --disable-doc --enable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libharfbuzz --enable-libfontconfig --enable-libopenh264 --enable-libdav1d --enable-cross-compile --arch=arm64 --target-os=darwin --cross-prefix=arm64-apple-darwin20.0.0- --host-cc=/Users/runner/miniforge3/conda-bld/ffmpeg_1735647017461/_build_env/bin/x86_64-apple-darwin13.4.0-clang --enable-neon --disable-gnutls --enable-libmp3lame --enable-libvpx --enable-libass --enable-pthreads --enable-libopenvino --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --enable-librsvg --pkg-config=/Users/runner/miniforge3/conda-bld/ffmpeg_1735647017461/_build_env/bin/pkg-config\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.100 / 61. 19.100\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "[aist#0:0/pcm_s16le @ 0x128607530] Guessed Channel Layout: mono\n",
      "Input #0, wav, from 'demo_tts.wav':\n",
      "  Duration: 00:00:10.49, bitrate: 384 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 24000 Hz, mono, s16, 384 kb/s\n",
      "Input #1, avi, from 'temp/result.avi':\n",
      "  Metadata:\n",
      "    software        : Lavf61.7.100\n",
      "  Duration: 00:00:10.37, start: 0.000000, bitrate: 3417 kb/s\n",
      "  Stream #1:0: Video: mpeg4 (Simple Profile) (DIVX / 0x58564944), yuv420p, 1280x720 [SAR 1:1 DAR 16:9], 3418 kb/s, 30 fps, 30 tbr, 30 tbn\n",
      "Stream mapping:\n",
      "  Stream #1:0 -> #0:0 (mpeg4 (native) -> h264 (libx264))\n",
      "  Stream #0:0 -> #0:1 (pcm_s16le (native) -> aac (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x1286095d0] -qscale is ignored, -crf is recommended.\n",
      "[libx264 @ 0x1286095d0] using SAR=1/1\n",
      "[libx264 @ 0x1286095d0] using cpu capabilities: ARMv8 NEON\n",
      "[libx264 @ 0x1286095d0] profile High, level 3.1, 4:2:0, 8-bit\n",
      "[libx264 @ 0x1286095d0] 264 - core 164 r3095 baee400 - H.264/MPEG-4 AVC codec - Copyleft 2003-2022 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=22 lookahead_threads=3 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'final_demo_lipsynced.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf61.7.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 1280x720 [SAR 1:1 DAR 16:9], q=2-31, 30 fps, 15360 tbn\n",
      "      Metadata:\n",
      "        encoder         : Lavc61.19.100 libx264\n",
      "      Side data:\n",
      "        cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "  Stream #0:1: Audio: aac (LC) (mp4a / 0x6134706D), 24000 Hz, mono, fltp, 69 kb/s\n",
      "      Metadata:\n",
      "        encoder         : Lavc61.19.100 aac\n",
      "[out#0/mp4 @ 0x600003740480] video:1905KiB audio:86KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.446431%\n",
      "frame=  311 fps=0.0 q=-1.0 Lsize=    2000KiB time=00:00:10.30 bitrate=1590.5kbits/s speed=16.6x    \n",
      "[libx264 @ 0x1286095d0] frame I:2     Avg QP:19.86  size: 55249\n",
      "[libx264 @ 0x1286095d0] frame P:170   Avg QP:21.25  size:  9157\n",
      "[libx264 @ 0x1286095d0] frame B:139   Avg QP:24.23  size:  2032\n",
      "[libx264 @ 0x1286095d0] consecutive B-frames: 33.4% 18.6%  6.8% 41.2%\n",
      "[libx264 @ 0x1286095d0] mb I  I16..4: 23.8% 71.7%  4.5%\n",
      "[libx264 @ 0x1286095d0] mb P  I16..4:  1.6%  4.2%  0.0%  P16..4: 35.2%  7.7%  4.9%  0.0%  0.0%    skip:46.3%\n",
      "[libx264 @ 0x1286095d0] mb B  I16..4:  0.4%  1.2%  0.0%  B16..8: 30.8%  1.1%  0.1%  direct: 1.0%  skip:65.5%  L0:46.9% L1:50.8% BI: 2.3%\n",
      "[libx264 @ 0x1286095d0] 8x8 transform intra:72.1% inter:80.4%\n",
      "[libx264 @ 0x1286095d0] coded y,uvDC,uvAC intra: 35.2% 66.9% 10.6% inter: 8.1% 15.6% 0.2%\n",
      "[libx264 @ 0x1286095d0] i16 v,h,dc,p: 31% 30% 23% 16%\n",
      "[libx264 @ 0x1286095d0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 23% 20% 49%  2%  1%  2%  1%  2%  2%\n",
      "[libx264 @ 0x1286095d0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 30% 30% 15%  3%  6%  5%  5%  3%  3%\n",
      "[libx264 @ 0x1286095d0] i8c dc,h,v,p: 39% 28% 27%  7%\n",
      "[libx264 @ 0x1286095d0] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x1286095d0] ref P L0: 70.5%  9.6% 14.1%  5.9%\n",
      "[libx264 @ 0x1286095d0] ref B L0: 84.9% 11.5%  3.6%\n",
      "[libx264 @ 0x1286095d0] ref B L1: 97.0%  3.0%\n",
      "[libx264 @ 0x1286095d0] kb/s:1504.53\n",
      "[aac @ 0x1286104b0] Qavg: 7065.523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint from: Wav2Lip/checkpoints/wav2lip.pth\n",
      "Model loaded\n",
      "Finished inference - Now to ffmpeg command\n",
      "Lip-synced video saved to: final_demo_lipsynced.mp4\n",
      "Final lip-synced video: final_demo_lipsynced.mp4\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T23:17:49.189486Z",
     "start_time": "2025-01-12T23:17:49.189421Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "384e0ef4064265ca",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Dubbing Env",
   "language": "python",
   "name": "my_dubbing_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
